{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 6\n",
    "\n",
    "### Character level language model\n",
    "\n",
    "The goal of this model is to be able to generate text that is similar to the training data.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "You will need to download the dataset from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt and place it into the *data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a vocabulary of tokens, the `CharacterTokenizer` is used. In this step the `fit()` extracts tokens from the previously imported data. Here, a token is represented by a single a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.preprocessing.text import CharacterTokenizer\n",
    "\n",
    "tknzr = CharacterTokenizer()\n",
    "tknzr.fit(data)\n",
    "tknzr.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tknzr.encode(data)\n",
    "data_enc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we need to build the training dataset. `num_samples` represents the number of inputs for the neural network. `block_size` defines, how many characters are considered when trying to predict the following one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the dataset for training, the following steps must be performed:\n",
    "- choose random samples from the data by randomly selecting a sequence of 8 characters for training and the 9th character as the target\n",
    "- one-hot-encode tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from walnut.preprocessing.encoding import one_hot_encode\n",
    "\n",
    "# initialize tensors with zeros\n",
    "X = walnut.zeros((num_samples, block_size, tknzr.vocab_size))\n",
    "Y = walnut.zeros((num_samples, tknzr.vocab_size))\n",
    "\n",
    "#randomly choose indices of blocks in the original data\n",
    "rand_indices = np.random.randint(0, len(data) - block_size, (num_samples,))\n",
    "\n",
    "for i, index in enumerate(rand_indices):\n",
    "    # get characters and the label from the data\n",
    "    context = data_enc[index : index + block_size]\n",
    "    label = walnut.match_dims(data_enc[index + block_size], 1)\n",
    "\n",
    "    # one-hot-encode indices and add to the tensors\n",
    "    X[i] = one_hot_encode(context, tknzr.vocab_size).data\n",
    "    Y[i] = one_hot_encode(label, tknzr.vocab_size).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(num_samples * 0.8)\n",
    "n2 = int(n1 + num_samples * 0.1)\n",
    "\n",
    "x_train = X[:n1]\n",
    "y_train = Y[:n1]\n",
    "x_val = X[n1:n2]\n",
    "y_val = Y[n1:n2]\n",
    "x_test = X[n2:]\n",
    "y_test = Y[n2:]\n",
    "\n",
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{x_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")\n",
    "print(f\"{x_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the neural network structure\n",
    "\n",
    "As our first layer, an `Embedding` is used. It assigns each token a n-dimensional vector. The vector's components are learned and updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "embed_dims = 30\n",
    "n_hidden = 100\n",
    "\n",
    "# stacked linear layers\n",
    "model = nn.Sequential(layers=[\n",
    "    nn.modules.Embedding(embed_dims, input_shape=(block_size, tknzr.vocab_size)),\n",
    "    nn.modules.Reshape(), # concatenate all token embeddings (batch, blocksize*dims)\n",
    "\n",
    "    nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "    nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "    nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "    nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "    nn.modules.Linear(tknzr.vocab_size, act=\"softmax\", norm=\"layer\")\n",
    "])\n",
    "\n",
    "# wavenet-like architecture\n",
    "# grouping = 2\n",
    "\n",
    "# model = nn.Sequential(layers=[\n",
    "#     nn.modules.Embedding(embed_dims, input_shape=(block_size, tknzr.vocab_size)), # (batch, block, dims)\n",
    "#     nn.modules.Layernorm(),\n",
    "\n",
    "#     # reduction layer 1: 8 --> 4 groups\n",
    "#     nn.modules.Reshape((-1, grouping * embed_dims)),\n",
    "#     nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "\n",
    "#     # reduction layer 2: 4 --> 2 groups\n",
    "#     nn.modules.Reshape((-1, grouping * n_hidden)),\n",
    "#     nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "\n",
    "#     # flatten, more linear layers to approx. match num of parameters\n",
    "#     nn.modules.Reshape(),\n",
    "#     nn.modules.Linear(n_hidden, act=\"tanh\", norm=\"layer\"),\n",
    "#     nn.modules.Linear(64, act=\"tanh\", norm=\"layer\"),\n",
    "#     nn.modules.Linear(tknzr.vocab_size, act=\"softmax\", norm=\"layer\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(nn.optimizers.Adam(l_r=1e-3), nn.losses.Crossentropy(), nn.metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model\n",
    "\n",
    "Since there are usually quite a large number of classes (=tokens) in language models, the training process is slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist, val_hist = model.train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    verbose=\"reduced\",\n",
    "    val_data=(x_val, y_val),\n",
    "    reset_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss above only represents one batch. Below the model is evaluated on the entire training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train)\n",
    "print(f'training loss {loss:.4f}')\n",
    "print(f'training accuracy {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `kaiming_he` is used as an initialization method, the activations do not get saturated (a.k.a. fall within the flat spots of the tanh function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = {f\"{i + 1} {l.__class__.__name__}\" : l.y.data.copy() for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(acts, figsize=(15, 3), title=\"activation distribution\", bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, because layer normalization is used, the activiation gradients follow a normal distribuition, even in deeper layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gradients = {f\"{i + 1} {l.__class__.__name__}\" : l.y.grad.copy() for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(act_gradients, figsize=(15, 3), title=\"activation gradient distribution\", bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to A. Karpathy (https://www.youtube.com/watch?v=P6sfmUTpUmc) the ratio between the parameter updates and their value should be around 1e-3 for a suitable learning rate. If the ratio is too high or low, the learning rate should be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for i, l in enumerate(model.layers):\n",
    "    if not isinstance(l, nn.modules.Linear):\n",
    "        continue\n",
    "    val = (walnut.Tensor(l.w.params[\"delta\"]).std() / l.w.std()).log10().item()\n",
    "    print(f\"{l.__class__.__name__} {i}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'loss {loss:.4f}')\n",
    "print(f'accuracy {accuracy:.4f}')\n",
    "\n",
    "# high scores:\n",
    "# linear stack: accuracy 0.4304 after 10000 epochs (32 min)\n",
    "# wave net-like: accuracy: 0.4058 after 5000 epochs (34 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate text\n",
    "To see the model in action, it is given a starting sequence. Then it is used to generate $n$ characters using previous charactes as input. The better the model is trained, the more sensical the output will be. This can take quite a lot of training though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = walnut.ones((block_size,)).astype(\"int\") # use ones as startig context\n",
    "\n",
    "for _ in range(300):\n",
    "    context_enc = one_hot_encode(context, tknzr.vocab_size) # encode tokens\n",
    "    context_enc = walnut.expand_dims(context_enc, 0) # create fake batch dim\n",
    "\n",
    "    pred = model(context_enc) # get model prediction for a character\n",
    "    index = walnut.choice(pred) # choose a character from prediction\n",
    "    print(tknzr.decode(walnut.expand_dims(index, 0)), end=\"\")\n",
    "\n",
    "    context = context.append(index, axis=0).astype(\"int\") # append predicted character\n",
    "    context = context[1:] # set new context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
