{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import walnut\n",
    "import walnut.tensor_utils as tu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.3\n",
    "\n",
    "### Language Model: Recurrent Neural network\n",
    "\n",
    "The neural network is able to predict the following character by looking at multiple previous ones. They do not, however, consider their position and context. The next step is to therefore use a recurrent neural network that allows individual sequence elements to communicate.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "Like in the bigram model, the tinyshakespeare dataset is used. (https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.preprocessing.text import CharacterTokenizer\n",
    "\n",
    "tknzr = CharacterTokenizer()\n",
    "tknzr.fit(data)\n",
    "tknzr.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tknzr.encode(data)\n",
    "data_enc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset\n",
    "In this example a larger `block_size` is now used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = walnut.zeros((num_samples, block_size))\n",
    "Y = walnut.zeros((num_samples, block_size))\n",
    "\n",
    "rand_indices = np.random.randint(0, len(data) - block_size - 1, (num_samples,))\n",
    "\n",
    "for i, index in enumerate(rand_indices):\n",
    "    context = data_enc[index : index + block_size]\n",
    "    label = data_enc[index + 1 : index + block_size + 1]\n",
    "\n",
    "    X[i] = context\n",
    "    Y[i] = label\n",
    "\n",
    "X = X.astype(\"int\")\n",
    "Y = Y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.99*X.len)\n",
    "\n",
    "x_train = X[:n1]\n",
    "y_train = Y[:n1]\n",
    "x_test = X[n1:]\n",
    "y_test = Y[n1:]\n",
    "\n",
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{x_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "As our first layer, again, an `Embedding` layer is used. It is followed by a stack of recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "from walnut.nn.layers import *\n",
    "from walnut.nn.blocks import *\n",
    "\n",
    "vocab_size = tknzr.vocab_size\n",
    "embed_dims = 10\n",
    "n_hidden = 256\n",
    "num_rec_layers = 3\n",
    "\n",
    "model = nn.Sequential([\n",
    "    Embedding(vocab_size, embed_dims),\n",
    "    Recurrent(embed_dims, n_hidden, num_layers=num_rec_layers),\n",
    "    Linear(n_hidden, vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.AdamW(3e-4),\n",
    "    loss_fn=nn.losses.Crossentropy(),\n",
    "    metric=nn.metrics.get_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.analysis import model_summary\n",
    "model_summary(model, (block_size,), \"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "train_loss_hist, val_loss_hist = model.train(x_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = model.evaluate(x_test, y_test)\n",
    "print(f'loss {loss:.4f}')\n",
    "\n",
    "# 1.9249 (2 epochs, 8 blocksize, 32 batches, 10 emb dims, 3 rec layers, 256 n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.funcional import softmax\n",
    "context = walnut.ones((1, block_size,)).astype(\"int\")\n",
    "\n",
    "for _ in range(1000):\n",
    "    pred = model(context)\n",
    "    index = walnut.choice(softmax(pred[:, -1]))\n",
    "    print(tknzr.decode(walnut.expand_dims(index, 0)), end=\"\")\n",
    "    context = context.append(tu.expand_dims(index, 0), axis=1).astype(\"int\")\n",
    "    context = context[:, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
