{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compyute: available devices ['cpu', 'cuda']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import compyute as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if cp.engine.gpu_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.3\n",
    "\n",
    "### Language Model: LSTM\n",
    "\n",
    "A dense neural network is not able to capture the sequential and time-dependent character of text. An alternative is the LSTM, which is able to memorize past tokens.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "Again, the tinyshakespeare dataset is used. (https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from compyute.preprocessing.text import BPETokenizer, save_tokenizer, load_tokenizer\n",
    "\n",
    "# tokenizer = BPETokenizer()\n",
    "# tokenizer.fit(data, vocab_size=400)\n",
    "# save_tokenizer(tokenizer, \"nn_tokenizer.cp\")\n",
    "tokenizer = load_tokenizer(\"nn_tokenizer.cp\")\n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617172"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_enc = tokenizer.encode(data)\n",
    "len(data_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(555447, 8)\n",
      "y_train.shape=(555447, 8)\n",
      "X_val.shape=(61717, 8)\n",
      "y_val.shape=(61717, 8)\n"
     ]
    }
   ],
   "source": [
    "X = cp.stack([data_enc[i : i + block_size] for i in range(len(data_enc) - block_size)])\n",
    "y = cp.Tensor([data_enc[i + 1 : i + 1 + block_size] for i in range(len(data_enc) - block_size)])\n",
    "\n",
    "n = int(len(X) * 0.9)\n",
    "\n",
    "X_train = X.int()[:n]\n",
    "y_train = y.int()[:n]\n",
    "X_val = X.int()[n:]\n",
    "y_val = y.int()[n:]\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "Now, `LSTM`-layers are used, followed by a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compyute.nn as nn\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dims = 32\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, embed_dims),\n",
    "\n",
    "    nn.Layernorm((block_size, embed_dims)),\n",
    "    nn.LSTM(embed_dims, 256),\n",
    "\n",
    "    nn.Layernorm((block_size, 256)),\n",
    "    nn.Linear(256, vocab_size)\n",
    ")\n",
    "\n",
    "model.to_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential\n",
      "---------------------------------------------------------------\n",
      "Layer                     Output Shape            # Parameters\n",
      "===============================================================\n",
      "Sequential                (-1, 8, 400)                  417168\n",
      " Embedding                (-1, 8, 32)                    12800\n",
      " Layernorm                (-1, 8, 32)                      512\n",
      " LSTM                     (-1, 8, 256)                  296960\n",
      " Layernorm                (-1, 8, 256)                    4096\n",
      " Linear                   (-1, 8, 400)                  102800\n",
      "===============================================================\n",
      "\n",
      "Total parameters: 417168\n"
     ]
    }
   ],
   "source": [
    "model.summary(input_shape=(block_size,), input_dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.nn.trainer import optimizers, Trainer\n",
    "from compyute.nn.trainer.callbacks import AdaptiveLR, EarlyStopping, History, ProgressBar\n",
    "\n",
    "history = History()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizers.Adam(lr=1e-3),\n",
    "    loss=\"cross_entropy\",\n",
    "    metric=\"accuracy\",\n",
    "    callbacks=[\n",
    "        history,\n",
    "        EarlyStopping(target=\"val_loss\", patience=5),\n",
    "        AdaptiveLR(target=\"val_loss\", epoch_range=3),\n",
    "        ProgressBar()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e4abe03e6746468290830aa90e494f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/136 [00:00<?, ? steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216e7aca5d3e4a46ade3067c7d9d2b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50:   0%|          | 0/136 [00:00<?, ? steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\nn\\trainer\\trainer.py:96\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, x_train, y_train, epochs, val_data, batch_size)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_training(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader():\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__callback(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_training(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\nn\\trainer\\trainer.py:176\u001b[0m, in \u001b[0;36mTrainer.__train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    173\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# compute loss and metrics\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__callback_cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(y_pred, y_batch)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\nn\\modules\\containers.py:184\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo modules have been added yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[1;32m--> 184\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(dy: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\nn\\modules\\module.py:101\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_y(y)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\nn\\modules\\layers\\recurrent.py:212\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    210\u001b[0m ifgo[:, t, :i2] \u001b[38;5;241m=\u001b[39m sigmoid(ifgo_preact[:, :i2])[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# input, forget\u001b[39;00m\n\u001b[0;32m    211\u001b[0m ifgo[:, t, i2:i3] \u001b[38;5;241m=\u001b[39m ifgo_preact[:, i2:i3]\u001b[38;5;241m.\u001b[39mtanh()  \u001b[38;5;66;03m# node\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m ifgo[:, t, i3:] \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mifgo_preact\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi3\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# cell state\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# c_t = f_t * c_t-1 + i_t * g_t\u001b[39;00m\n\u001b[0;32m    216\u001b[0m c[:, t] \u001b[38;5;241m=\u001b[39m ifgo[:, t, i1:i2] \u001b[38;5;241m*\u001b[39m c[:, t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m ifgo[:, t, :i1] \u001b[38;5;241m*\u001b[39m ifgo[:, t, i2:i3]\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\nn\\functional.py:136\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x, return_backward_fn)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(\n\u001b[0;32m    118\u001b[0m     x: Tensor, return_backward_fn: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Tensor, Optional[Callable[[Tensor], Tensor]]]:\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the sigmoid function.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m        Backward function.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mexp()) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    137\u001b[0m     backward \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m dy: (y \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y)) \u001b[38;5;241m*\u001b[39m dy) \u001b[38;5;28;01mif\u001b[39;00m return_backward_fn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y, backward\n",
      "File \u001b[1;32mc:\\Users\\xboxd\\Source\\Compyute\\examples\\..\\compyute\\tensor.py:668\u001b[0m, in \u001b[0;36mTensor.exp\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexp\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    667\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Exponential of tensor element.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__e\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 4096\n",
    "\n",
    "trainer.train(X_train, y_train, epochs=epochs, batch_size=batch_size, val_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(t1, t2):\n",
    "    trace1 = history[t1]\n",
    "    trace2 = history[t2]\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(cp.arange(start=1, stop=len(trace1) + 1).to_numpy(), trace1, linewidth=1)\n",
    "    plt.plot(cp.arange(start=1, stop=len(trace2) + 1).to_numpy(), trace2, linewidth=1)\n",
    "    plt.legend([t1, t2])\n",
    "    plt.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "plot_history(\"val_loss\", \"val_accuracy_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"You shall \"\n",
    "print(context, end=\"\")\n",
    "\n",
    "context = tokenizer.encode(context)\n",
    "context = context.pad((block_size - len(context), 0)).reshape((1, -1))\n",
    "context.to_device(model.device)\n",
    "\n",
    "for _ in range(250):\n",
    "    pred = model(context).squeeze()\n",
    "    index = pred[-1].argmax(-1)\n",
    "    char = tokenizer.decode([index.item()])\n",
    "    print(char, end=\"\")\n",
    "    context = context.append(index.reshape((1, 1)), axis=1).int()\n",
    "    context = context[:, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
