{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import compyute as cp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3.1\n",
    "\n",
    "### Deep neural network using sequential model\n",
    "\n",
    "The goal of this model is to classify iris species based on numerical features.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "You will need to download the dataset from https://www.kaggle.com/datasets/uciml/iris and place it into the *data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_orig = pd.read_csv('../data/iris.csv')\n",
    "data = data_orig.copy()\n",
    "data.drop(columns=['Id'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are categorical values. To be used in the model, all data needs to be numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = data.copy()\n",
    "data_enc[\"Species\"] = data_enc[\"Species\"].astype(\"category\").cat.codes\n",
    "data_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = cp.Tensor(data_enc.to_numpy())\n",
    "data_tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is split into a training, validation and a testing dataset using the `split_train_test_val_data()` to evaluate the model later on. Before splitting the data is also shuffled, since sometimes raw data is sorted in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.preprocessing import split_train_val_test\n",
    "\n",
    "train, val, test = split_train_val_test(data_tensor, ratio_val=0.2, ratio_test=0.2)\n",
    "train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and labels are now seperated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train[:, :-1], train[:, -1].int()\n",
    "X_val, y_val = val[:, :-1], val[:, -1].int()\n",
    "X_test, y_test = test[:, :-1], test[:, -1].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks tend to run into problems if values are very high. Therefore it is common to normalize the data. This can be done using the `normalize()` function, which applies min-max feature scaling to a tensor.<br><br>\n",
    "$ X'=a+\\frac{(X-X_{min})\\cdot(b-a)}{X_{max}-X_{min}} $<br><br>, where<br><br>$ a $ ... lower bound<br>$ b $ ... upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.preprocessing import normalize\n",
    "\n",
    "X_train = normalize(X_train, axis=0, l_bound=-1)\n",
    "X_val = normalize(X_val, axis=0, l_bound=-1)\n",
    "X_test = normalize(X_test, axis=0, l_bound=-1)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{X_train.shape=}')\n",
    "print (f'{y_train.shape=}')\n",
    "\n",
    "print (f'{X_val.shape=}')\n",
    "print (f'{y_val.shape=}')\n",
    "\n",
    "print (f'{X_test.shape=}')\n",
    "print (f'{y_test.shape=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the neural network structure\n",
    "Here the individual layers of the neural network models are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compyute.nn as nn\n",
    "from compyute.nn.layers import *\n",
    "\n",
    "model = nn.SequentialModel([\n",
    "    Linear(4, 16),\n",
    "    ReLU(),\n",
    "    Batchnorm1d(16),\n",
    "    Linear(16, 3),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is compiled to finalize the model. Besides the SGD optimizer, the framework also provides other algorithms like Adam. There are also multiple loss functions to choose from. Since this example explores a classification problem, the cross entropy loss function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.SGD(),\n",
    "    loss_fn=nn.losses.Crossentropy(),\n",
    "    metric_fn=nn.metrics.accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.nn.analysis import model_summary\n",
    "model_summary(model, (4,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_losses, train_scores, val_losses, val_scores = model.train(\n",
    "    X_train, y_train, epochs=200, val_data=(X_val, y_val), verbose=1, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {\n",
    "    \"train_losses\" : train_losses,\n",
    "    \"train_scores\" : train_scores\n",
    "}\n",
    "\n",
    "nn.analysis.plot_curve(traces=traces, figsize=(15, 3), title=\"train history\", x_label=\"steps\", y_label=\"loss/accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {\n",
    "    \"val_losses\" : val_losses,\n",
    "    \"val_scores\" : val_scores\n",
    "}\n",
    "\n",
    "nn.analysis.plot_curve(traces=traces, figsize=(15, 3), title=\"val history\", x_label=\"steps\", y_label=\"loss/accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "Using the defined metric, the model's performance can be evaluated using testing/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'loss {loss:.4f}')\n",
    "print(f'accuracy {100*accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deee277ef8cb4a05cf6441d551c854fa5e547ddedbca2c10e6f5685ea62b6c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
