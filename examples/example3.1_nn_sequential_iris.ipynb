{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import pandas as pd\n",
    "import walnut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3.1\n",
    "\n",
    "### Deep neural network using sequential model\n",
    "\n",
    "The goal of this model is to classify iris species based on numerical features.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "You will need to download the dataset from https://www.kaggle.com/datasets/uciml/iris and place it into the *data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = pd.read_csv('../data/iris.csv')\n",
    "data = data_orig.copy()\n",
    "data.drop(columns=['Id'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are categorical values. To be used in the model, all data needs to be numerical. The function `pd_categorical_to_numeric()` can be used to one-hot-encode all categorical data of a Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = walnut.preprocessing.encoding.pd_categorical_to_numeric(data, columns=[\"Species\"])\n",
    "data_enc.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is split into a training, validation and a testing dataset using the `split_train_test_val_data()` to evaluate the model later on. Before splitting the data is also shuffled, since sometimes raw data is sorted in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = walnut.df_to_tensor(data_enc)\n",
    "t_train, t_val, t_test = walnut.preprocessing.split_train_val_test(tensor)\n",
    "t_train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and labels are now seperated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = t_train[:, :-1], t_train[:, -1].astype(\"int\")\n",
    "x_val, y_val = t_val[:, :-1], t_val[:, -1].astype(\"int\")\n",
    "x_test, y_test = t_test[:, :-1], t_test[:, -1].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks tend to run into problems if values are very high. Therefore it is common to normalize the data. This can be done using the `normalize()` function, which applies min-max feature scaling to a tensor.<br><br>\n",
    "$ X'=a+\\frac{(X-X_{min})\\cdot(b-a)}{X_{max}-X_{min}} $<br><br>, where<br><br>$ a $ ... lower bound<br>$ b $ ... upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = walnut.preprocessing.normalize(x_train, axis=0)\n",
    "x_val = walnut.preprocessing.normalize(x_val, axis=0)\n",
    "x_test = walnut.preprocessing.normalize(x_test, axis=0)\n",
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'{x_train.shape=}')\n",
    "print (f'{y_train.shape=}')\n",
    "\n",
    "print (f'{x_val.shape=}')\n",
    "print (f'{y_val.shape=}')\n",
    "\n",
    "print (f'{x_test.shape=}')\n",
    "print (f'{y_test.shape=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the neural network structure\n",
    "Here the individual layers of the neural network models are defined. If the weights for a layer are not definied, they are initialized randomly. For experimental purposes, they can be manually initialized using various initialization methods, such as `kaiming_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "from walnut.nn.layers import *\n",
    "from walnut.nn.inits import *\n",
    "\n",
    "n_hidden = 100\n",
    "gain = get_gain(\"tanh\")\n",
    "\n",
    "init = normal\n",
    "model = nn.Sequential([\n",
    "    Linear(4, n_hidden, weights=init((4, n_hidden))), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, weights=init((n_hidden, n_hidden))), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, weights=init((n_hidden, n_hidden))), Tanh(),\n",
    "    Linear(n_hidden, 3, weights=init((n_hidden, 3)))\n",
    "])\n",
    "\n",
    "# init = kaiming_normal\n",
    "# model = nn.Sequential([\n",
    "#     Linear(4, n_hidden, weights=init((4, n_hidden), gain)), Tanh(),\n",
    "#     Linear(n_hidden, n_hidden, weights=init((n_hidden, n_hidden), gain)), Tanh(),\n",
    "#     Linear(n_hidden, n_hidden, weights=init((n_hidden, n_hidden), gain)), Tanh(),\n",
    "#     Linear(n_hidden, 3, weights=init((n_hidden, 3)))\n",
    "# ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is compiled to finalize the model. Besides the SGD optimizer, the framework also provides other algorithms like Adam. There are also multiple loss functions to choose from. Since this example explores a classification problem, the cross entropy loss function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.SGD(),\n",
    "    loss_fn=nn.losses.Crossentropy(),\n",
    "    metric=nn.metrics.get_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.analysis import model_summary\n",
    "model_summary(model, (4,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist, val_loss_hist = model.train(x_train, y_train, epochs=100, val_data=(x_val, y_val), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {\n",
    "    \"train_loss\" : train_loss_hist,\n",
    "    \"val_loss\" : val_loss_hist\n",
    "}\n",
    "\n",
    "nn.analysis.plot_curve(traces=traces, figsize=(15, 3), title=\"loss history\", x_label=\"epoch\", y_label=\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "Using the defined metric, the model's performance can be evaluated using testing/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'loss {loss:.4f}')\n",
    "print(f'accuracy {100*accuracy:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze the model\n",
    "Usind different plots, the models performance and training behaviour can be analyzed (eg. checking for overfitting)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `normal` weight initialization method is used, the **tanh** activations get saturated very fast and the gradients \"die out\". If other initializers, such as `kaiming_normal` are used, this couteracts this behaviour. Furthermore the initial loss is lower and the model is therefore not wasting time correcting unnecessary high weight values in the beginning (Analysis inspired by Andrej Karpathy - highly recommend checking out his videos on YouTube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {f\"{i + 1} {l.__class__.__name__}\" : l.y.data.copy() for i, l in enumerate(model.layers[0].layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(activations, figsize=(15, 3), title=\"activation distribution\", bins=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the gradient of saturated neurons get very close to zero. If that happens for all batches, then the neuron is not learning and it is considererd dead (white pixels in the plot). By using the Kaiming He initialization method this can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturations = {f\"{i + 1} {l.__class__.__name__}\" : (l.y.abs() > 0.99).data for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_images(saturations, (150, 30), \"gray\", plot_axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_gradients = {f\"{i + 1} {l.__class__.__name__}\" : l.w.grad for i, l in enumerate(model.layers[0].layers) if l.__class__.__name__ == \"Linear\"}\n",
    "nn.analysis.plot_distrbution(weight_gradients, figsize=(15, 3), title=\" weight gradient distribution\", bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gradients = {f\"{i + 1} {l.__class__.__name__}\" : l.y.grad for i, l in enumerate(model.layers[0].layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(act_gradients, figsize=(15, 3), title=\"activation gradient distribution\", bins=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deee277ef8cb4a05cf6441d551c854fa5e547ddedbca2c10e6f5685ea62b6c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
