{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import walnut\n",
    "import walnut.tensor_utils as tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if walnut.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.2\n",
    "\n",
    "### Language Model: Neural network\n",
    "\n",
    "The bigram model is able to predict the following character by looking at the previous one. For better predictions it helps to not only consider one character for a prediction. In this example a neural network is used that uses multiple characters for predictions.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "Like in the bigram model, the tinyshakespeare dataset is used. (https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.preprocessing.text import CharacterTokenizer\n",
    "\n",
    "tknzr = CharacterTokenizer()\n",
    "tknzr.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tknzr.encode(data)\n",
    "len(data_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset\n",
    "In this example a larger `block_size` is now used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = walnut.zeros((num_samples, block_size))\n",
    "y = walnut.zeros((num_samples, block_size))\n",
    "\n",
    "rand_indices = np.random.randint(0, len(data_enc) - block_size - 1, (num_samples,))\n",
    "\n",
    "for i, index in enumerate(rand_indices):\n",
    "    context = data_enc[index : index + block_size]\n",
    "    label = data_enc[index + 1 : index + block_size + 1]\n",
    "\n",
    "    X[i] = context\n",
    "    y[i] = label\n",
    "\n",
    "X_train = X.int()\n",
    "y_train = y.int()[:,-1]\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "As our first layer, again, an `Embedding` layer is used. It is followed by a stack of linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "from walnut.nn.layers import *\n",
    "\n",
    "vocab_size = tknzr.vocab_size\n",
    "embed_dims = 30\n",
    "n_hidden = 256\n",
    "dtype = \"float32\"\n",
    "\n",
    "model = nn.Sequential([\n",
    "    Embedding(vocab_size, embed_dims, dtype=dtype),\n",
    "    Flatten(),\n",
    "    Linear(block_size*embed_dims, n_hidden, use_bias=False, dtype=dtype),\n",
    "    Layernorm((n_hidden,), dtype=dtype),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden, use_bias=False, dtype=dtype),\n",
    "    Layernorm((n_hidden,), dtype=dtype),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden, use_bias=False, dtype=dtype),\n",
    "    Layernorm((n_hidden,), dtype=dtype),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden, use_bias=False, dtype=dtype),\n",
    "    Layernorm((n_hidden,), dtype=dtype),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, n_hidden, use_bias=False, dtype=dtype),\n",
    "    Layernorm((n_hidden,), dtype=dtype),\n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size, dtype=dtype)\n",
    "])\n",
    "\n",
    "model.to_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.AdamW(1e-4, eps=1e-8, beta2=0.95),\n",
    "    loss_fn=nn.losses.Crossentropy(eps=1e-8),\n",
    "    metric_fn=nn.metrics.accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.analysis import model_summary\n",
    "model_summary(model, (block_size,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32000\n",
    "\n",
    "train_losses, train_scores, _, _ = model.train(X_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.funcional import softmax\n",
    "context = walnut.ones((1, block_size,), device=device)\n",
    "\n",
    "for _ in range(1000):\n",
    "    pred = model(context).squeeze()\n",
    "    index = walnut.random_choice_indices(softmax(pred.float()))  # needs at least float32 otherwise probs might not sum to 1\n",
    "    print(tknzr.decode(walnut.expand_dims(index, 0)), end=\"\")\n",
    "    context = context.append(tu.expand_dims(index, 0), axis=1)\n",
    "    context = context[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# get numpy array of embedding table\n",
    "embs = model.sub_modules[0].sub_modules[0].w.cpu().data\n",
    "\n",
    "# reduce dimensions to 2 to make\n",
    "tsne = TSNE(random_state=0).fit_transform(embs)\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x=tsne[:,0], y=tsne[:,1], alpha=0.5, s=100)\n",
    "plt.axis(\"off\")\n",
    "for i in range(len(tsne)):\n",
    "    char = tknzr.decode(walnut.Tensor([i], dtype=\"int\"))\n",
    "    plt.text(x=tsne[i,0]-0.03, y=tsne[i,1]-0.04, s=char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
