{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import compyute as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.2\n",
    "\n",
    "### Language Model: Neural network\n",
    "\n",
    "The bigram model is able to predict the following character by looking at the previous one. For better predictions it helps to not only consider one character for a prediction. In this example a neural network is used that uses multiple characters for predictions.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "Like in the bigram model, the tinyshakespeare dataset is used. (https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "\n",
    "This time, a Byte-Pair-Encoding tokenizer is used to allow for more information to be passed into the neural net without increasing the context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.preprocessing.text import BPETokenizer, save_tokenizer, load_tokenizer\n",
    "\n",
    "# tokenizer = BPETokenizer()\n",
    "# tokenizer.fit(data, vocab_size=400)\n",
    "# save_tokenizer(tokenizer, \"nn_tokenizer.cp\")\n",
    "tokenizer = load_tokenizer(\"nn_tokenizer.cp\")\n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tokenizer.encode(data)[:5000]\n",
    "len(data_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset\n",
    "In this example a larger `block_size` is now used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cp.zeros((num_samples, block_size))\n",
    "y = cp.zeros((num_samples, block_size))\n",
    "\n",
    "rand_indices = cp.random.uniform_int((num_samples,), 0, len(data_enc) - block_size - 1, )\n",
    "\n",
    "for i, index in enumerate(rand_indices):\n",
    "    context = data_enc[index : index + block_size]\n",
    "    label = data_enc[index + 1 : index + block_size + 1]\n",
    "\n",
    "    X[i] = context\n",
    "    y[i] = label\n",
    "\n",
    "n = int(len(X) * 0.9)\n",
    "\n",
    "X_train = X.int()[:n]\n",
    "y_train = y.int()[:n,-1]\n",
    "X_val = X.int()[n:]\n",
    "y_val = y.int()[n:,-1]\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "As our first layer, again, an `Embedding` layer is used. It is followed by a stack of linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compyute.nn as nn\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dims = 30\n",
    "n_hidden = 256\n",
    "\n",
    "model = nn.SequentialModel([\n",
    "    nn.Embedding(vocab_size, embed_dims),\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(block_size*embed_dims, n_hidden, use_bias=False),\n",
    "    nn.Layernorm((n_hidden,)),\n",
    "    nn.Tanh(),\n",
    "\n",
    "    nn.Linear(n_hidden, n_hidden, use_bias=False),\n",
    "    nn.Layernorm((n_hidden,)),\n",
    "    nn.Tanh(),\n",
    "\n",
    "    nn.Linear(n_hidden, n_hidden, use_bias=False),\n",
    "    nn.Layernorm((n_hidden,)),\n",
    "    nn.Tanh(),\n",
    "    \n",
    "    nn.Linear(n_hidden, vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Compyute` also includes a few methods to decay the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.model_summary(model, (block_size,), \"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.nn.trainer import callbacks, optimizers, losses, metrics, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizers.AdamW(lr=1e-2),\n",
    "    loss_functon=losses.Crossentropy(),\n",
    "    metric_function=metrics.Accuracy(),\n",
    "    callbacks=[callbacks.lr_decay.CosineLR(lr_min=1e-3, until_epoch=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.retain_values = True\n",
    "history = trainer.train(X_train, y_train, epochs=5, verbose=2, val_data=(X_val, y_val), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(losses, scores, label):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(cp.arange(start=1, stop=len(losses) + 1).to_numpy(), losses, linewidth=1)\n",
    "    plt.plot(cp.arange(start=1, stop=len(scores) + 1).to_numpy(), scores, linewidth=1)\n",
    "    plt.title(f\"{label} history\")\n",
    "    plt.legend([f\"{label}_loss\", f\"{label}_score\"])\n",
    "    plt.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.state[\"train_losses\"], trainer.state[\"train_scores\"], \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(trainer.state[\"val_losses\"], trainer.state[\"val_scores\"], \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze the model\n",
    "Usind different plots, the models performance and training behaviour can be analyzed (eg. checking for overfitting)\n",
    "\n",
    "If the `normal` weight initialization method is used, the **tanh** activations get saturated very fast and the gradients \"die out\". If other initializers, such as `kaiming_normal` are used, this couteracts this behaviour. Furthermore the initial loss is lower and the model is therefore not wasting time correcting unnecessary high weight values in the beginning (Analysis inspired by Andrej Karpathy - highly recommend checking out his videos on YouTube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "def plot_distrbution(ys):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    legends = []\n",
    "    for y in ys:\n",
    "        label, array = y\n",
    "        mean = array.mean()\n",
    "        std = array.std()\n",
    "        print(f\"{label:10s} | mean {mean:9.4f} | std {std:9.4f}\")\n",
    "        y_vals, x_vals = numpy.histogram(array, bins=n_hidden, density=True)\n",
    "        x_vals = numpy.delete(x_vals, -1)\n",
    "        plt.plot(x_vals, y_vals, linewidth=1)\n",
    "        legends.append(f\"{label:s}\")\n",
    "    plt.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.legend(legends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_weights = [\n",
    "    (f\"{l.__class__.__name__}{i}\", l.w.cpu().to_numpy())\n",
    "    for i, l in enumerate(model.child_modules[0].child_modules)\n",
    "    if l.__class__.__name__ == \"Linear\"\n",
    "]\n",
    "plot_distrbution(lin_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_activations = [\n",
    "    (f\"{l.__class__.__name__}{i}\", l.y.cpu().to_numpy())\n",
    "    for i, l in enumerate(model.child_modules[0].child_modules)\n",
    "    if l.__class__.__name__ == \"Tanh\"\n",
    "]\n",
    "plot_distrbution(tanh_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_saturations = [\n",
    "    (f\"{l.__class__.__name__}{i}\", l.y.cpu().abs().to_numpy() > 0.99)\n",
    "    for i, l in enumerate(model.child_modules[0].child_modules)\n",
    "    if l.__class__.__name__ == \"Tanh\"\n",
    "]\n",
    "\n",
    "for label, image in tanh_saturations:\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.xlabel(label)\n",
    "    plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the gradient of saturated neurons get very close to zero. If that happens for all batches, then the neuron is not learning and it is considererd dead (white pixels in the plot). By using the Kaiming He initialization method this can be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_gradients = [\n",
    "    (f\"{l.__class__.__name__}{i}\", l.w.grad.to_numpy())\n",
    "    for i, l in enumerate(model.child_modules[0].child_modules)\n",
    "    if l.__class__.__name__ == \"Linear\"\n",
    "]\n",
    "plot_distrbution(weight_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_gradients = [\n",
    "    (f\"{l.__class__.__name__}{i}\", l.y.grad.to_numpy())\n",
    "    for i, l in enumerate(model.child_modules[0].child_modules)\n",
    "    if l.__class__.__name__ == \"Tanh\"\n",
    "]\n",
    "plot_distrbution(activation_gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
