{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compyute as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.2\n",
    "\n",
    "### Language Model: Neural network\n",
    "\n",
    "The bigram model is able to predict the following character by looking at the previous one. For better predictions it helps to not only consider one character for a prediction. In this example a neural network is used that uses multiple characters for predictions.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "Like in the bigram model, the tinyshakespeare dataset is used. (https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "\n",
    "This time, a Byte-Pair-Encoding tokenizer is used to allow for more information to be passed into the neural net without increasing the context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.preprocessing.text import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train new tokenizer\n",
    "# tokenizer.fit(data, vocab_size=1024)\n",
    "# cp.save(tokenizer.get_state_dict(), \"small_tokenizer.cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer_state = cp.load(\"small_tokenizer.cp\")\n",
    "tokenizer.load_state_dict(tokenizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tokenizer.encode(data)\n",
    "len(data_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset\n",
    "In this example a larger `block_size` is now used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = cp.tensor(data_enc, dtype=cp.int32)\n",
    "X = cp.stack([data_enc[i * block_size : i * block_size + block_size] for i in range(len(data_enc) // block_size - 1)])\n",
    "y = cp.stack([data_enc[i * block_size + block_size] for i in range(len(data_enc) // block_size - 1)])\n",
    "\n",
    "X, idx = cp.random.shuffle(X)\n",
    "y = y[idx]\n",
    "\n",
    "X_train = X\n",
    "y_train = y\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "As our first layer, again, an `Embedding` layer is used. It is followed by a stack of linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute import nn\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_dims = 10\n",
    "n_hidden = 128\n",
    "\n",
    "weight_init = \"kaiming_normal\"\n",
    "\n",
    "emb = nn.Embedding(vocab_size, embed_dims)\n",
    "lin = nn.Linear(n_hidden, vocab_size)\n",
    "nn.utils.initializers.get_initializer(weight_init, \"tanh\")(lin.w)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    emb,\n",
    "    nn.Flatten(),\n",
    "    nn.DenseBlock(block_size*embed_dims, n_hidden, activation=\"tanh\", weight_init=weight_init),\n",
    "    nn.DenseBlock(n_hidden, n_hidden, activation=\"tanh\", weight_init=weight_init),\n",
    "    nn.DenseBlock(n_hidden, n_hidden, activation=\"tanh\", weight_init=weight_init),\n",
    "    nn.DenseBlock(n_hidden, n_hidden, activation=\"tanh\", weight_init=weight_init),\n",
    "    nn.DenseBlock(n_hidden, n_hidden, activation=\"tanh\", weight_init=weight_init),\n",
    "    lin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = cp.nn.utils.get_module_summary(model, input_shape=(block_size,), input_dtype=cp.int32)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model\n",
    "\n",
    "To avoid overfitting the model, the `EarlyStopping`-Callback can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "dl = nn.utils.Dataloader((X, y), batch_size=batch_size)\n",
    "optim = nn.optimizers.Adam(model.get_parameters())\n",
    "loss_fn = nn.CrossEntropy()\n",
    "\n",
    "model.retain_values = True\n",
    "\n",
    "for e in range(epochs):\n",
    "    # training\n",
    "    model.training()\n",
    "    for x, y in dl():\n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "        _ = loss_fn(y_pred, y)\n",
    "\n",
    "        # backward pass\n",
    "        model.compute_grads(loss_fn.compute_grads())  # compute new gradients\n",
    "        optim.step()  # update parameters\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze the model\n",
    "Usind different plots, the models performance and training behaviour can be analyzed (eg. checking for overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distrbution(ys):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    for y in ys:\n",
    "        label, tensor = y\n",
    "        mean = cp.mean(tensor).item()\n",
    "        std = cp.std(tensor).item()\n",
    "        print(f\"{label:10s} | mean {mean:9.4f} | std {std:9.4f}\")\n",
    "        y_vals, x_vals = cp.histogram(tensor, bins=n_hidden, density=True)\n",
    "        plt.plot(x_vals[:-1], y_vals, linewidth=1)\n",
    "        legends.append(f\"{label:s}\")\n",
    "    plt.legend(legends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_weights = [(m.label + str(i), p.to_cpu()) for i, m in enumerate(model.get_modules()) for p in m.get_parameters(False) if p.ndim > 1 and m.label == \"Linear\"]\n",
    "plot_distrbution(lin_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_activations = [(m.label + str(i), m.y.to_cpu()) for i, m in enumerate(model.get_modules()) if m.label == \"Tanh\"]\n",
    "plot_distrbution(tanh_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_saturations = [(m.label + str(i), cp.abs(m.y.to_cpu()) > 0.99) for i, m in enumerate(model.get_modules()) if m.label == \"Tanh\"]\n",
    "\n",
    "for label, image in tanh_saturations:\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.xlabel(label)\n",
    "    plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the gradient of saturated neurons get very close to zero. If that happens for all batches, then the neuron is not learning and it is considererd dead (white pixels in the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_weight_grads = [(m.label + str(i), p.grad.to_cpu()) for i, m in enumerate(model.get_modules()) for p in m.get_parameters(False) if p.ndim > 1 and m.label == \"Linear\"]\n",
    "plot_distrbution(lin_weight_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_gradients = [(m.label + str(i), m.y.grad.to_cpu()) for i, m in enumerate(model.get_modules()) if m.label == \"Tanh\"]\n",
    "plot_distrbution(activation_gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
