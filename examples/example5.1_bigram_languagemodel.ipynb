{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import compyute as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.1\n",
    "\n",
    "### Language Model: Bigram model\n",
    "\n",
    "The goal of this model is to be able to generate text that is similar to the training data using a single character to predict the next one.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "The dataset can be downloaded from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt. Afterwards, it needs to be placed it into the */data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "To train a neural network on text, it needs to be represented by numerical values. For this reason a tokenizer is used. To build a vocabulary of tokens, here the `CharacterTokenizer` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.preprocessing.text import CharacterTokenizer\n",
    "\n",
    "tknzr = CharacterTokenizer()\n",
    "tknzr.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every character is assigned an integer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers can be used to encode text and decode token ids. Here, the string \"Hello World!\" is encoded and afterwards represented by the respective token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hello World!\"\n",
    "encoded_string = tknzr.encode(string)\n",
    "encoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor of token ids can also be decoded again to get the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tknzr.decode([i]) for i in encoded_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr.decode(encoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the data to train a model, it needs to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tknzr.encode(data)\n",
    "data_enc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset\n",
    "Next up, we need to build the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cp.Tensor([data_enc[i] for i in range(len(data_enc) - 1)])\n",
    "y = cp.Tensor([data_enc[i + 1] for i in range(len(data_enc) - 1)])\n",
    "\n",
    "X_shuffle, idx = cp.random.shuffle(X)\n",
    "y_shuffle = y[idx]\n",
    "\n",
    "n = int(len(X) * 0.9)\n",
    "\n",
    "X_train = X_shuffle.int()[:n, None]\n",
    "y_train = y_shuffle.int()[:n]\n",
    "X_val = X_shuffle.int()[n:, None]\n",
    "y_val = y_shuffle.int()[n:]\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "An `Embedding` layer is used to assign each token an n-dimensional vector. The vector's components are then learned and updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compyute.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(tknzr.vocab_size, tknzr.vocab_size),\n",
    "    nn.Flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(input_shape=(1,), input_dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model\n",
    "\n",
    "Since there are usually quite a large number of classes (=tokens) in language models, the training process can be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.nn.trainer import losses, metrics, optimizers, Trainer\n",
    "from compyute.nn.trainer.callbacks import ProgressBar\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizers.AdamW(lr=1e-2),\n",
    "    loss=losses.CrossEntropy(),\n",
    "    metric=metrics.Accuracy(),\n",
    "    callbacks=[ProgressBar()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get numpy array of embedding table\n",
    "embs = model.modules[0].w.cpu().data\n",
    "\n",
    "# reduce dimensions to 2 to make\n",
    "tsne = TSNE(random_state=0).fit_transform(embs)\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x=tsne[:,0], y=tsne[:,1], alpha=0.5, s=100)\n",
    "plt.axis(\"off\")\n",
    "for i in range(len(tsne)):\n",
    "    char = tknzr.decode(cp.Tensor([i]))\n",
    "    plt.text(x=tsne[i,0]-0.1, y=tsne[i,1]-0.15, s=char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Generate text\n",
    "To see the model in action, it is given a starting character (here token 1). Then it is used to generate $n$ characters using previous charactes as input. The better the model is trained, the more sensical the output will be. This can take quite a lot of training though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = cp.ones((1, 1,), dtype=\"int32\") # use 1 as startig context\n",
    "n = 1000\n",
    "\n",
    "for _ in range(n):\n",
    "    pred = model(context).squeeze() # predict following character using the current context\n",
    "    probs, _ = cp.nn.functional.softmax(pred)\n",
    "    index = cp.random.multinomial(\n",
    "        x=pred.shape[-1],\n",
    "        p=probs,\n",
    "        shape=(1,)\n",
    "    ) # choose a character from prediction\n",
    "    print(tknzr.decode([index.item()]), end=\"\")\n",
    "    context = context.append(index[None, :], axis=1).int() # append predicted character to context\n",
    "    context = context[:, 1:] # set new context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
