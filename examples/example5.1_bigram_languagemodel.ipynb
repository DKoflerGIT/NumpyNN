{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # for sibling import\n",
    "\n",
    "import walnut\n",
    "import walnut.tensor_utils as tu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.1\n",
    "\n",
    "### Language Model: Bigram model\n",
    "\n",
    "The goal of this model is to be able to generate text that is similar to the training data using a single character to predict the next one.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "The dataset can be downloaded from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt. Afterwards, it needs to be placed it into the */data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "To train a neural network on text, it needs to be represented by numerical values. For this reason a tokenizer is used. To build a vocabulary of tokens, here the `CharacterTokenizer` is used. In this step `fit()` is used to extract tokens from the previously imported data. Every character is assigned an integer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.preprocessing.text import CharacterTokenizer\n",
    "\n",
    "tknzr = CharacterTokenizer()\n",
    "tknzr.fit(data)\n",
    "tknzr.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tokenizer has built a vocabulary, it can be used to encode and decode data. Here, the string \"Hello World!\" is encoded and afterwards represented by the respective tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hello World!\"\n",
    "encoded_string = tknzr.encode(string)\n",
    "encoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor of tokens can also be decoded again to get the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr.decode(encoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the data to train a model, it needs to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = tknzr.encode(data)\n",
    "data_enc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build dataset\n",
    "Next up, we need to build the training dataset. `num_samples` represents the number of samples for the neural network. `block_size` defines how many characters are considered when trying to predict the following one. Since this is a bigram model, only one character is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "block_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataset, samples are taken from the data by randomly selecting a character as an input and the following character as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize tensors with zeros\n",
    "X = walnut.zeros((num_samples, block_size))\n",
    "Y = walnut.zeros((num_samples, block_size))\n",
    "\n",
    "#randomly choose indices of blocks in the original data\n",
    "rand_indices = np.random.randint(0, len(data) - block_size - 1, (num_samples,))\n",
    "\n",
    "for i, index in enumerate(rand_indices):\n",
    "    # get characters and the label from the data\n",
    "    context = data_enc[index : index + block_size]\n",
    "    label = data_enc[index + 1 : index + block_size + 1]\n",
    "\n",
    "    # one-hot-encode indices and add to the tensors\n",
    "    X[i] = context\n",
    "    Y[i] = label\n",
    "\n",
    "X = X.astype(\"int\")\n",
    "Y = Y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.99*X.len)\n",
    "\n",
    "x_train = X[:n1]\n",
    "y_train = Y[:n1]\n",
    "x_test = X[n1:]\n",
    "y_test = Y[n1:]\n",
    "\n",
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{x_test.shape=}\")\n",
    "print(f\"{y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the neural network structure\n",
    "\n",
    "An `Embedding` layer is used to assign each token an n-dimensional vector. The vector's components are then learned and updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "from walnut.nn.layers import *\n",
    "\n",
    "vocab_size = tknzr.vocab_size\n",
    "\n",
    "model = nn.Sequential([\n",
    "    Embedding(vocab_size, vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.AdamW(3e-4),\n",
    "    loss_fn=nn.losses.Crossentropy(),\n",
    "    metric=nn.metrics.get_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.analysis import model_summary\n",
    "model_summary(model, (block_size,), \"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the model\n",
    "\n",
    "Since there are usually quite a large number of classes (=tokens) in language models, the training process can be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "train_loss_hist, val_loss_hist = model.train(x_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = model.evaluate(x_test, y_test)\n",
    "print(f'loss {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generate text\n",
    "To see the model in action, it is given a starting character (here token 1). Then it is used to generate $n$ characters using previous charactes as input. The better the model is trained, the more sensical the output will be. This can take quite a lot of training though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from walnut.nn.funcional import softmax\n",
    "context = walnut.ones((1, block_size,)).astype(\"int\") # use 1 as startig context\n",
    "\n",
    "n = 1000\n",
    "\n",
    "for _ in range(n):\n",
    "    pred = model(context).squeeze() # predict following character using the current context\n",
    "    index = walnut.random_choice_indices(softmax(pred)) # choose a character from prediction\n",
    "    print(tknzr.decode(walnut.expand_dims(index, 0)), end=\"\")\n",
    "    context = context.append(tu.expand_dims(index, 0), axis=1).astype(\"int\") # append predicted character to context\n",
    "    context = context[:, 1:] # set new context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
