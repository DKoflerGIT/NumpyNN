{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyute.functional import zeros, zeros_like\n",
    "from compyute.nn.module import Module\n",
    "from compyute.nn.parameter import Parameter\n",
    "from compyute.random import uniform\n",
    "from compyute.tensor import Tensor\n",
    "from compyute.types import ArrayLike\n",
    "\n",
    "\n",
    "class RecurrentCell(Module):\n",
    "    \"\"\"Recurrent cell.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, h_channels: int, use_bias: bool = True, dtype: str = \"float32\") -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.h_channels = h_channels\n",
    "        self.use_bias = use_bias\n",
    "        self.dtype = dtype\n",
    "\n",
    "        k = in_channels**-0.5\n",
    "\n",
    "        # input gate\n",
    "        w_i = uniform((in_channels, h_channels), -k, k)\n",
    "        self.w_i = Parameter(w_i, dtype=dtype, label=\"w_i\")\n",
    "        u_i = uniform((in_channels, h_channels), -k, k)\n",
    "        self.u_i = Parameter(u_i, dtype=dtype, label=\"u_i\")\n",
    "        if use_bias:\n",
    "            b_i = zeros((h_channels,))\n",
    "            self.b_i = Parameter(b_i, dtype=dtype, label=\"b_i\")\n",
    "\n",
    "        # forget gate\n",
    "        w_f = uniform((in_channels, h_channels), -k, k)\n",
    "        self.w_f = Parameter(w_f, dtype=dtype, label=\"w_f\")\n",
    "        u_f = uniform((in_channels, h_channels), -k, k)\n",
    "        self.u_f = Parameter(u_f, dtype=dtype, label=\"u_f\")\n",
    "        if use_bias:\n",
    "            b_f = zeros((h_channels,))\n",
    "            self.b_f = Parameter(b_f, dtype=dtype, label=\"b_f\")\n",
    "            \n",
    "        # output gate\n",
    "        w_o = uniform((in_channels, h_channels), -k, k)\n",
    "        self.w_o = Parameter(w_o, dtype=dtype, label=\"w_o\")\n",
    "        u_o = uniform((in_channels, h_channels), -k, k)\n",
    "        self.u_o = Parameter(u_o, dtype=dtype, label=\"u_o\")\n",
    "        if use_bias:\n",
    "            b_o = zeros((h_channels,))\n",
    "            self.b_o = Parameter(b_o, dtype=dtype, label=\"b_o\")\n",
    "            \n",
    "        # cell\n",
    "        w_c = uniform((in_channels, h_channels), -k, k)\n",
    "        self.w_c = Parameter(w_c, dtype=dtype, label=\"w_c\")\n",
    "        u_c = uniform((in_channels, h_channels), -k, k)\n",
    "        self.u_c = Parameter(u_c, dtype=dtype, label=\"u_c\")\n",
    "        if use_bias:\n",
    "            b_c = zeros((h_channels,))\n",
    "            self.b_c = Parameter(b_c, dtype=dtype, label=\"b_c\")\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        self.check_dims(x, [3])\n",
    "        x = x.astype(self.dtype)\n",
    "\n",
    "        # input gate\n",
    "        f_t_h = \n",
    "\n",
    "        # iterate over timesteps\n",
    "        h = zeros_like(x_h, device=self.device)\n",
    "        for t in range(x_h.shape[1]):\n",
    "            h_t = h[:, t - 1] @ self.w_h\n",
    "            if self.use_bias:\n",
    "                h_t += self.b_h\n",
    "\n",
    "            # activation\n",
    "            h[:, t] = (x_h[:, t] + h_t).tanh()\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            def backward(dy: ArrayLike) -> ArrayLike:\n",
    "                dh = dy.astype(self.dtype)\n",
    "                self.set_dy(dh)\n",
    "\n",
    "                dx_h = zeros_like(x_h, device=self.device).data\n",
    "                self.w_h.grad = zeros_like(self.w_h, device=self.device).data\n",
    "\n",
    "                for t in range(x.shape[1] - 1, -1, -1):\n",
    "                    # add hidden state grad of next t, if not last t\n",
    "                    if t == x_h.shape[1] - 1:\n",
    "                        out_grad = dh[:, t]\n",
    "                    else:\n",
    "                        out_grad = dh[:, t] + dx_h[:, t + 1] @ self.w_h.T\n",
    "\n",
    "                    # activation grads\n",
    "                    dx_h[:, t] = (1 - h.data[:, t] ** 2) * out_grad\n",
    "\n",
    "                    # hidden weight grads\n",
    "                    if t > 0:\n",
    "                        self.w_h.grad += h[:, t - 1].T @ dx_h[:, t]\n",
    "\n",
    "                # hidden bias grads\n",
    "                self.b_h.grad = dx_h.sum((0, 1))\n",
    "\n",
    "                # input grads\n",
    "                dx = dx_h @ self.w_i.T\n",
    "\n",
    "                # input weight grads\n",
    "                dw = x.transpose().data @ dx_h\n",
    "                self.w_i.grad = dw.sum(axis=0)\n",
    "\n",
    "                # input bias grads\n",
    "                self.b_i.grad = dx_h.sum((0, 1))\n",
    "\n",
    "                return dx\n",
    "\n",
    "            self.backward = backward\n",
    "\n",
    "        self.set_y(h)\n",
    "        return h\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
