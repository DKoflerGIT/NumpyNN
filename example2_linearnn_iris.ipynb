{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import walnut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "\n",
    "### Deep neural network using multiple linear layers\n",
    "\n",
    "### Step 1: Prepare data\n",
    "You will need to download the dataset from https://www.kaggle.com/datasets/uciml/iris and place it into the *data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = pd.read_csv('data/iris.csv')\n",
    "data = data_orig.copy()\n",
    "data.drop(columns=['Id'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are categorical values. To be used in the model, all data needs to be numerical. The function `categorical_to_numeric()` can be used to one-hot-encode all categorical data of a Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = walnut.preprocessing.pd_one_hot_encode(data, columns=['Species'])\n",
    "data_enc.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is split into a training, validation and a testing dataset using the `split_train_test_val_data()` to evaluate the model later on. Before splitting the data is also shuffled, since sometimes raw data is sorted in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = walnut.pd_to_tensor(data_enc)\n",
    "t_train, t_val, t_test = walnut.preprocessing.split_train_val_test(tensor)\n",
    "t_train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and labels are now seperated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = walnut.preprocessing.split_features_labels(t_train, num_x_cols=4)\n",
    "x_val, y_val = walnut.preprocessing.split_features_labels(t_val, num_x_cols=4)\n",
    "x_test, y_test = walnut.preprocessing.split_features_labels(t_test, num_x_cols=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks tend to run into problems if values are very high. Therefore it is common to normalize the data. This can be done using the `normalize()` function, which applies min-max feature scaling to a tensor.<br><br>\n",
    "$ X'=a+\\frac{(X-X_{min})\\cdot(b-a)}{X_{max}-X_{min}} $<br><br>, where<br><br>$ a $ ... lower bound<br>$ b $ ... upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = walnut.preprocessing.normalize(x_train, axis=0)\n",
    "x_val = walnut.preprocessing.normalize(x_val, axis=0)\n",
    "x_test = walnut.preprocessing.normalize(x_test, axis=0)\n",
    "x_train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the neural network structure\n",
    "Here the individual layers of the neural network models are defined. For linear layers, activation functions and weight initialization methods can be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "from walnut.nn import layers\n",
    "\n",
    "#using weight initialization following a normal distribution\n",
    "model = nn.Sequential(layers=[\n",
    "    layers.Linear(16, input_shape=(4,), act=\"tanh\", init=\"normal\"),\n",
    "    layers.Linear(16, act=\"tanh\", init=\"normal\"),\n",
    "    layers.Linear(16, act=\"tanh\", init=\"normal\"),\n",
    "    layers.Linear(3, act=\"softmax\", init=\"normal\")\n",
    "])\n",
    "\n",
    "# using kaiming he initializaton method\n",
    "# model = nn.Sequential(layers=[\n",
    "#     layers.Linear(16, input_shape=(4,), act=\"tanh\", init=\"kaiming_he\"),\n",
    "#     layers.Linear(16, act=\"tanh\", init=\"kaiming_he\"),\n",
    "#     layers.Linear(16, act=\"tanh\", init=\"kaiming_he\"),\n",
    "#     layers.Linear(3, act=\"softmax\", init=\"kaiming_he\")\n",
    "# ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is compiled to internally connect it's layers and initialize the model. The SGD optimizer provides an optional momentum term and nesterov momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.SGD(l_r=1e-2, momentum=0.9, nesterov=True),\n",
    "    loss_fn=nn.losses.Crossentropy(),\n",
    "    metric=nn.metrics.Accuracy()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.train(x_train, y_train, epochs=100, val_data=(x_val, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "Using the defined metric, the model's performance can be evaluated using testing/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('loss', loss)\n",
    "print('accuracy', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze the model\n",
    "Usind different plots, the models performance and training behaviour can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.analysis.plot_curve(hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `random` weight initialization method is used, the tanh activations get saturated very fast and the gradients \"die out\". If `kaiming_he` is used this couteracts this behaviour. Furthermore the initial loss is lower and the model ist therefore not wasting time correcting unnecessary high weight values in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {f\"{i + 1} {l.__class__.__name__}\" : l.y.data.copy() for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(activations, title=\"activation distribution\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = {f\"{i + 1} {l.__class__.__name__}\" : l.y.grad.copy() for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Linear\"}\n",
    "nn.analysis.plot_distrbution(gradients, title=\"gradient distribution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deee277ef8cb4a05cf6441d551c854fa5e547ddedbca2c10e6f5685ea62b6c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
