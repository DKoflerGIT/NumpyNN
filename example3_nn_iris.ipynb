{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import walnut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3\n",
    "\n",
    "### Deep neural network using multiple linear layers\n",
    "\n",
    "The goal of this model is to classify iris species based on numerical features.\n",
    "\n",
    "### Step 1: Prepare data\n",
    "You will need to download the dataset from https://www.kaggle.com/datasets/uciml/iris and place it into the *data* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = pd.read_csv('data/iris.csv')\n",
    "data = data_orig.copy()\n",
    "data.drop(columns=['Id'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are categorical values. To be used in the model, all data needs to be numerical. The function `categorical_to_numeric()` can be used to one-hot-encode all categorical data of a Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = walnut.preprocessing.encoding.pd_one_hot_encode(data, columns=['Species'])\n",
    "data_enc.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the data is split into a training, validation and a testing dataset using the `split_train_test_val_data()` to evaluate the model later on. Before splitting the data is also shuffled, since sometimes raw data is sorted in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = walnut.pd_to_tensor(data_enc)\n",
    "t_train, t_val, t_test = walnut.preprocessing.split_train_val_test(tensor)\n",
    "t_train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and labels are now seperated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = walnut.preprocessing.split_features_labels(t_train, num_x_cols=4)\n",
    "x_val, y_val = walnut.preprocessing.split_features_labels(t_val, num_x_cols=4)\n",
    "x_test, y_test = walnut.preprocessing.split_features_labels(t_test, num_x_cols=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks tend to run into problems if values are very high. Therefore it is common to normalize the data. This can be done using the `normalize()` function, which applies min-max feature scaling to a tensor.<br><br>\n",
    "$ X'=a+\\frac{(X-X_{min})\\cdot(b-a)}{X_{max}-X_{min}} $<br><br>, where<br><br>$ a $ ... lower bound<br>$ b $ ... upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = walnut.preprocessing.normalize(x_train, axis=0)\n",
    "x_val = walnut.preprocessing.normalize(x_val, axis=0)\n",
    "x_test = walnut.preprocessing.normalize(x_test, axis=0)\n",
    "x_train[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the neural network structure\n",
    "Here the individual layers of the neural network models are defined. For linear layers, activation functions and weight initialization methods can be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import walnut.nn as nn\n",
    "\n",
    "# init = \"uniform\"\n",
    "# init = \"normal\"\n",
    "# init = \"xavier_uniform\"\n",
    "# init = \"xavier_normal\"\n",
    "# init = \"kaiming_uniform\"\n",
    "init = \"kaiming_normal\"\n",
    "\n",
    "model = nn.Sequential([\n",
    "    nn.layers.Linear(32, input_shape=(4,), act=\"tanh\", init=init),\n",
    "    nn.layers.Linear(32, act=\"tanh\", init=init),\n",
    "    nn.layers.Linear(32, act=\"tanh\", init=init),\n",
    "    nn.layers.Linear(3, act=\"softmax\", init=init)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is compiled to internally connect it's layers and initialize the model. The SGD optimizer provides an optional momentum term and nesterov momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.SGD(),\n",
    "    loss_fn=nn.losses.Crossentropy(),\n",
    "    metric=nn.metrics.Accuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist, val_loss_hist = model.train(x_train, y_train, epochs=200, val_data=(x_val, y_val), reset_params=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "Using the defined metric, the model's performance can be evaluated using testing/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'loss {loss:.4f}')\n",
    "print(f'accuracy {100*accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_test)\n",
    "nn.analysis.plot_confusion_matrix(predictions, y_test, (3, 3), cmap='Blues')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze the model\n",
    "Usind different plots, the models performance and training behaviour can be analyzed (eg. checking for overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "traces = {\n",
    "    \"train_loss\" : [l if i < n else sum(train_loss_hist[i-(n-1):i+1])/n for i,l in enumerate(train_loss_hist)],\n",
    "    \"val_loss\" : val_loss_hist\n",
    "}\n",
    "nn.analysis.plot_curve(traces=traces, figsize=(15, 3), title=\"loss history\", x_label=\"epoch\", y_label=\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `random` weight initialization method is used, the **tanh** activations get saturated very fast and the gradients \"die out\". If other initializers, such as `kaiming_normal` are used, this couteracts this behaviour. Furthermore the initial loss is lower and the model is therefore not wasting time correcting unnecessary high weight values in the beginning (Analysis inspired by Andrej Karpathy - highly recommend checking out his videos on YouTube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {f\"{i + 1} {l.__class__.__name__}\" : l.y.data.copy() for i, l in enumerate(model.modules) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(activations, figsize=(15, 3), title=\"activation distribution\", bins=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the gradient of saturated neurons get very close to zero. If that happens for all batches, then the neuron is not learning and it is considererd dead (white pixels in the plot). By using the Kaiming He initialization method this can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturations = {f\"{i + 1} {l.__class__.__name__}\" : (l.y.abs() > 0.99).data for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_images(saturations, (150, 30), \"gray\", plot_axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_gradients = {f\"{i + 1} {l.__class__.__name__}\" : l.w.grad.copy() for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Linear\"}\n",
    "nn.analysis.plot_distrbution(weight_gradients, figsize=(15, 3), title=\" weight gradient distribution\", bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gradients = {f\"{i + 1} {l.__class__.__name__}\" : l.y.grad.copy() for i, l in enumerate(model.layers) if l.__class__.__name__ == \"Tanh\"}\n",
    "nn.analysis.plot_distrbution(act_gradients, figsize=(15, 3), title=\"activation gradient distribution\", bins=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deee277ef8cb4a05cf6441d551c854fa5e547ddedbca2c10e6f5685ea62b6c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
